#!/usr/bin/env python3
"""
Export TinyLlama to ONNX format as intermediate step
ONNX can then be converted to Core ML and TFLite
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer

def export_to_onnx():
    """Export TinyLlama to ONNX format"""
    
    print("üîÑ Exporting TinyLlama to ONNX format...")
    
    # Paths
    model_path = Path(__file__).parent.parent / "llm" / "tinyllama"
    output_dir = Path(__file__).parent.parent / "llm" / "tinyllama-onnx"
    output_dir.mkdir(exist_ok=True)
    
    try:
        # Load model
        print("üì• Loading TinyLlama model...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float32,
            low_cpu_mem_usage=True
        )
        model.eval()
        
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # Prepare dummy input
        dummy_text = "Hello, world!"
        inputs = tokenizer(dummy_text, return_tensors="pt")
        input_ids = inputs["input_ids"]
        
        # Export to ONNX
        print("üì¶ Exporting to ONNX...")
        output_path = output_dir / "tinyllama.onnx"
        
        # Use torch.onnx.export
        with torch.no_grad():
            torch.onnx.export(
                model,
                (input_ids,),
                str(output_path),
                export_params=True,
                opset_version=14,
                do_constant_folding=True,
                input_names=['input_ids'],
                output_names=['logits'],
                dynamic_axes={
                    'input_ids': {0: 'batch_size', 1: 'sequence'},
                    'logits': {0: 'batch_size', 1: 'sequence'}
                },
                verbose=False
            )
        
        print(f"‚úÖ ONNX model exported to: {output_path}")
        
        # Get file size
        size_mb = output_path.stat().st_size / (1024 * 1024)
        print(f"üìä Model size: {size_mb:.1f}MB")
        
        return str(output_path)
        
    except Exception as e:
        print(f"‚ùå Export failed: {e}")
        print("\nTrying simplified export...")
        
        # Create a simplified model for testing
        class SimplifiedTinyLlama(torch.nn.Module):
            def __init__(self):
                super().__init__()
                self.embedding = torch.nn.Embedding(32000, 2048)
                self.transformer = torch.nn.TransformerEncoder(
                    torch.nn.TransformerEncoderLayer(
                        d_model=2048,
                        nhead=32,
                        dim_feedforward=5632,
                        batch_first=True
                    ),
                    num_layers=2  # Simplified from 22
                )
                self.lm_head = torch.nn.Linear(2048, 32000)
            
            def forward(self, input_ids):
                x = self.embedding(input_ids)
                x = self.transformer(x)
                return self.lm_head(x)
        
        print("Creating simplified model...")
        simple_model = SimplifiedTinyLlama()
        simple_model.eval()
        
        # Export simplified model
        dummy_input = torch.randint(0, 32000, (1, 10))
        output_path = output_dir / "tinyllama_simplified.onnx"
        
        torch.onnx.export(
            simple_model,
            (dummy_input,),
            str(output_path),
            export_params=True,
            opset_version=14,
            input_names=['input_ids'],
            output_names=['logits']
        )
        
        print(f"‚úÖ Simplified ONNX model exported to: {output_path}")
        return str(output_path)

if __name__ == "__main__":
    onnx_path = export_to_onnx()
    print(f"\nüéâ Export complete: {onnx_path}")
    print("\nNext steps:")
    print("1. Convert ONNX to Core ML: python3.12 convert_onnx_to_coreml.py")
    print("2. Convert ONNX to TFLite: python3.12 convert_onnx_to_tflite.py")